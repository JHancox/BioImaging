{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncorking the I/O Bottleneck of Bio-Imaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Analysis of Whole Slide Image Features\n",
    "\n",
    "Now that we have reduced the dimensionality of our image data to a more manageable size we can load up the features that were saved as a Pandas dataframe in the previous lab and take a look at a few of the methods that are suitable for this type of data. Rather than using the standard Pandas dataframe, we are going to use the RAPIDS equivalent, cuDF (CUDA dataframe). This loads the data into GPU memory rather than using the host system's RAM. As you will see, this opens up a new realm of possibilities because of the huge speed boost this can provide.\n",
    "\n",
    "As usual, we start by importing the libraries that we'll need. As you will notice, there are a few new names here, such as cuDF, cuGraph and cuML. These are the core of the RAPIDS tools offering GPU accelerated Dataframe functionality, GPU accelerated graph analytics and GPU accelerated Machine Learning routines\n",
    "\n",
    "You will find documentation on all of these libraries and features here https://docs.rapids.ai/api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#from sklearn.decomposition import PCA\n",
    "from cuml import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import cudf\n",
    "import cugraph\n",
    "import cuml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the wsi_dfx file, which is a dataframe containing all of the features from the tiles from the last Notebook. Once we have loaded this, you can see that we immediately create a cdDF version of the dataframe, which puts it on to the GPU. The cdf.head command loads the first few rows and you can see that we have 32 variables named latent var (0-31) and an X and Y coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data we saved in our other notebook\n",
    "df = pd.read_pickle('data/wsi_dfx')\n",
    "\n",
    "#create a cuda dataframe from the pandas one\n",
    "cdf = cudf.from_pandas(df)\n",
    "\n",
    "cdf.head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also display some summary statistics for the dataframe columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some stats on the data\n",
    "cdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the principle components of these feature vectors, which is another way to distill the important information in these features into fewer variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the column names\n",
    "feat_cols = cdf.columns\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(df[feat_cols[0:32]].values)\n",
    "pca_df = cudf.DataFrame(pca_result,columns=[\"pca-1\", \"pca-2\", \"pca-3\"])\n",
    "\n",
    "print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_))\n",
    "print(pca_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are a few things going on in that last cell. Let's unpick what we did. \n",
    "\n",
    "Firstly we created a tuple of columns - feat_cols, which we used to get the first 32 column names and their values\n",
    "\n",
    "We created a [PCA analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) with 3 principle components. Then we used the PCA's fit_transform method to actually determine the 3 principle components.\n",
    "\n",
    "Finally, we created a new dataframe and provided names for the new PCA columns\n",
    "\n",
    "To check your understanding:\n",
    "\n",
    "1) Try printing the first 10 columns in the feat_cols and determine what data type is being used. \n",
    "\n",
    "2) Print out the memory_usage of the dataframe and see how much memory the Index adds (see the documentation here https://docs.rapids.ai/api/cudf/stable/api_docs/api/cudf.DataFrame.memory_usage.html)\n",
    "\n",
    "3) See if you can generate a new PCA analysis with 4 principle components, but only using the first 16 columns and the first 10,000 rows\n",
    "\n",
    "([Solution](solutions/solution3_1.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - use this cell to answer the questions above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another great visualisation tool is a confusion matrix with heatmap, which shows the correlation between the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (34,34)\n",
    "sns.set(rc={'figure.figsize':figsize}, style = 'white')\n",
    "\n",
    "data_corr = df.corr()\n",
    "ax =sns.heatmap(data=round(data_corr,3),\n",
    "    cmap=sns.diverging_palette(150, 275, s=80, l=55, n=9), annot=True,fmt='.1g', linewidth = .5,annot_kws={\"size\":8})\n",
    "b, t = plt.ylim()\n",
    "b += 0.5\n",
    "t -= 0.5\n",
    "plt.ylim(b,t)\n",
    "plt.show()\n",
    "\n",
    "# delete the dataframe to conserve memory\n",
    "del(data_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you should observe is that the purple diagonal is showing that each feature is perfectly correlated with itself (as you'd expect!). You can also see that there are a few features that do seem to be correlated with each other.\n",
    "\n",
    "To check your understanding, which is the only latent feature that seems to have any correlation to the x or y coordinate? \n",
    "\n",
    "(answer: Latent Var 30 with x coordinate)\n",
    "\n",
    "We can also plot the Principle Components in a scatter plot. You will see that we are using Numba as the means of getting the numpy array into a cuda dataframe. Numba is a Python-based function compiler that can create multi-core or GPU accelerated versions of ordinary Python code. In this instance we are just using it to convert a numpy array into a GPU memory equivalent.\n",
    "\n",
    "You will also notice that we are importing cuxfilter, which is another RAPIDS component which provides visualization capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuxfilter import DataFrame as fdf\n",
    "from cuxfilter.charts import scatter\n",
    "\n",
    "cdf_pca = cudf.DataFrame(pca_result, columns=[\"pca-1\", \"pca-2\", \"pca-3\"])\n",
    "\n",
    "cuxdf = fdf.from_dataframe(cdf_pca)\n",
    "scatter_chart = scatter(x=\"pca-1\", y=\"pca-3\", pixel_shade_type=\"linear\")\n",
    "\n",
    "cuxdf.dashboard([scatter_chart])\n",
    "scatter_chart.view()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other types of plot we can do, including 3D plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Matplotlib - which means we need to copy the data \n",
    "# back to the CPU - hence the .to_numpy() calls\n",
    "rndperm = np.random.permutation(pca_df.to_numpy().shape[0])\n",
    "\n",
    "pca_df.loc[rndperm,:][\"pca-1\"]\n",
    "\n",
    "ax = plt.figure(figsize=(16,10)).add_subplot(projection='3d')\n",
    "ax.scatter(\n",
    "    xs=pca_df.loc[rndperm,:][\"pca-1\"].to_numpy(), \n",
    "    ys=pca_df.loc[rndperm,:][\"pca-2\"].to_numpy(), \n",
    "    zs=pca_df.loc[rndperm,:][\"pca-3\"].to_numpy(), \n",
    "   c=pca_df.loc[rndperm,:][\"pca-3\"].to_numpy(), \n",
    "   cmap='tab10'\n",
    ")\n",
    "ax.set_xlabel('pca-1')\n",
    "ax.set_ylabel('pca-2')\n",
    "ax.set_zlabel('pca-3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the visualization is not showing us anything particularly insightful, but these types of plot can be very useful for gaining insights into data\n",
    "\n",
    "A more interesting idea to explore would be how the tiles on the whole slide image relate to one another in terms of the features that we calculated with our VAE (i.e. the latent features). Often it can be insightful to classify image features, such as nuclei and then build a graph in which each nucleus is a node and is linked by edges to its nearest neighbors.\n",
    "\n",
    "In order to turn our data into some sort of graph, we can use the nearest neighbour algorithm in cuML to find the nearest neighbours in feature space.\n",
    "With around 0.5 million items, each with 32 features, finding the 5 nearest neighbours is a sizeable computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.neighbors import NearestNeighbors as cuNN\n",
    "\n",
    "knn_cuml = cuNN()\n",
    "no_xy=cdf[feat_cols[0:32]]\n",
    "knn_cuml.fit(no_xy)\n",
    "\n",
    "%time D_cuml, I_cuml = knn_cuml.kneighbors(no_xy, 5)\n",
    "I_cuml, D_cuml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what this has produced is a list of the 5 nearest neighbours for each tile in latent feature space - since we only used the first 32 columns in the dataframe, thereby excluding the x and y coordinates. So what do we mean by 'nearest neighbout in feature space'? You can think of each feature as an axis and the value of each feature places the parent tile somewhere along this axis. The KNN (k-nearest neighbours) algorithm is using a Euclidean distance calculation which tells us how close each node is to every other node. Because we chose 5 as the number of nearest neighbours, we have a row value which represents the index of each node and then five columns containing the indexes of the 5 nearest nodes, with the nearest in column 0 and the furthest in column 4. You will also notice that the index in the nearest column, 0, always matches the row index. That's because the algorithm does not exclude each node from being its own nearest neighbour. We can ignore that column.\n",
    "\n",
    "We are looking at the indexes in the I_cuml dataframe. To look at the feature-space distances you should look at the D_cuml dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to compare this with the sklearn CPU implementation, be aware that it can take > 30 minutes to run! It is not necessary to execute it - the code is there as a reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this and the following cell to compare with CPU version\n",
    "from sklearn.neighbors import NearestNeighbors as skNN\n",
    "\n",
    "df_1=df[feat_cols[:32]]\n",
    "knn_sk = skNN(algorithm='brute',n_jobs=1)\n",
    "knn_sk.fit(df_1)\n",
    "\n",
    "# Only uncomment the lines below if you have a spare 30 mins to wait for it to complete!\n",
    "#%time D_sk, I_sk = knn_sk.kneighbors(df_1, 5)\n",
    "#I_sk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to convert the output of the KNN operation into a graph, we need to prepare the data. The data needs to be presented to the RAPIDS graph library, cugraph, as a set of edges with the source and destination node and an optional weight parameter.\n",
    "\n",
    "Firstly we combine the nearest neighbour indexes and distances into one dataframe and give them unique column names. We do this so that we can use the distance to set the weight of the connection between the nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give the columns names because they have to be unique in the merged dataframe\n",
    "I_cuml.columns=['ix1','n1','n2','n3','n4']\n",
    "D_cuml.columns=['ix2','d1','d2','d3','d4']\n",
    "all_cols = cudf.concat([I_cuml, D_cuml],axis=1)\n",
    "\n",
    "# remove the index and distance from the self-referenced nearest neighbour\n",
    "all_cols = all_cols[['n1','n2','n3','n4','d1','d2','d3','d4']]\n",
    "\n",
    "all_cols "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the next step is to manipulate this data so that it is in the desired format. There should be 3 columns, named 'source', 'target' and 'weight'.\n",
    "\n",
    "To do this, you will need to extract 4 sets of columns - one for each neighbour - and then concatenate the rows into a new dataframe.\n",
    "\n",
    "Remember that each row index represents a node, the n1-n4 columns contain the row index of a destination node and the d1-d4 columns contain the distance between these nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat the data to match the way edges are defined in cuGraph\n",
    "all_cols['index1'] = all_cols.index\n",
    "\n",
    "c1 = all_cols[['index1','n1','d1']]\n",
    "c1.columns=['source','target','weight']\n",
    "c2 = all_cols[['index1','n2','d2']]\n",
    "c2.columns=['source','target','weight']\n",
    "c3 = all_cols[['index1','n3','d3']]\n",
    "c3.columns=['source','target','weight']\n",
    "c4 = all_cols[['index1','n4','d4']]\n",
    "#c4 = all.iloc[:,[0, 4, 9]]\n",
    "c4.columns=['source','target','weight']\n",
    "                 \n",
    "edges = [c1,c2,c3,c4]\n",
    "\n",
    "edge_df = cudf.concat(edges)\n",
    "\n",
    "# remove the old dataframe from memory\n",
    "del(all_cols)\n",
    "\n",
    "edge_df = edge_df.reset_index()\n",
    "edge_df = edge_df[['source','target','weight']]\n",
    "edge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, if we are going to use the weight to set the distance between the nodes then we will need to invert the weight value, otherwise we are setting the nodes with greatest distance between them as having stronger connections, which the opposite of what we want. To do this we need to manipulate the data in the dataframe.\n",
    "\n",
    "See if you can invert the weights by manipulating the data in the edge_df DataFrame ([solution](solutions/solution3_2.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Invert the weight values so that larger distances create weaker weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe is now ready to be used to generate the graph. For this we use the cugraph library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can actually create a graoh!!\n",
    "G = cugraph.Graph()\n",
    "\n",
    "%time G.from_cudf_edgelist(edge_df,source='source', destination='target', edge_attr='weight', renumber=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the graph we can do standard graph analytical operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we can do some graph things\n",
    "count = cugraph.triangles(G)\n",
    "print(\"No of triangles = \" + str(count))\n",
    "\n",
    "coreno = cugraph.core_number(G)\n",
    "print(\"Core Number = \" + str(coreno))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cugraph is also able to use the force_atlas2 algorithm to generate a layout for the graph, which can allow us to visualise it. This algorithm uses a physics-based approach, treating the weights between the nodes as springs. The output is a new dataframe with the x and y coordinates of each node, which can then be fed into a visualiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_ = cugraph.layout.force_atlas2(G, max_iter=500,\n",
    "                strong_gravity_mode=False,\n",
    "                outbound_attraction_distribution=True,\n",
    "                lin_log_mode=False,\n",
    "                barnes_hut_theta=0.5, verbose=True)\n",
    "nodes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we use cuXFilter to render the whole graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cuxfilter.charts as cfc\n",
    "import cuxfilter.layouts as clo\n",
    "\n",
    "cux_df = fdf.load_graph((nodes_, edge_df))\n",
    "\n",
    "chart0 = cfc.graph(edge_color_palette=['gray', 'black'],\n",
    "                                            timeout=200, \n",
    "                                            node_aggregate_fn='mean', node_pixel_shade_type='linear',\n",
    "                                            edge_render_type='direct',#other option available -> 'curved'\n",
    "                                            edge_transparency=0.5\n",
    "                                          )\n",
    "d = cux_df.dashboard([chart0], layout=clo.double_feature)\n",
    "\n",
    "# draw the graph\n",
    "chart0.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to use the mouse-wheel to zoom in and out of the graph plot. If you zoom in far enough you will see the individual vertices (coloured) and edges (grey lines)\n",
    "\n",
    "As a follow-up exercise. Can you generate a similar plot that shows the 2 nearest neighbours of each tile using their X and Y coordinates rather than feature space? See if you can generate one plot for the graph of nearest neighbours using the force atlas computation and one on which the tiles are shown in their actual locations.\n",
    "\n",
    "If you do it correctly, then the latter should resemble the actual slide. If you zoom in, you may notice that the connections to the nearest neighbours are not always quite as you might expect. This is because, by default, the nearest neighbour algorithm uses heuristics to make the calculation faster. However, you can force it to use use the brute force algorithm (see if you can find out how to do that from the documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.neighbors import NearestNeighbors as cuNN\n",
    "import cuxfilter.charts as cfc\n",
    "import cuxfilter.layouts as clo\n",
    "import pickle\n",
    "\n",
    "# This loads the tiles created in the previous lab\n",
    "with open(\"data/tiles_xy\", \"rb\") as fp: \n",
    "    tiles_xy = pickle.load(fp)\n",
    "    \n",
    "tiles_xy_cdf=cudf.DataFrame(tiles_xy)\n",
    "\n",
    "#TODO Complete the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([Solution](solutions/solution3_last.py)) \n",
    "\n",
    "We hope you enjoyed this course and discovered a few new techniques to apply to your own Bio-imaging challenges!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
