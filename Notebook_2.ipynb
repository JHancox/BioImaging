{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncorking the I/O Bottleneck of Bio-Imaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Feeding your DL Pipelines with DASK\n",
    "\n",
    "In the first section, you saw how we can speed up the loading and decoding of large images by using a high-performance image loader such as cuCIM. You also saw how we can use multi-threading and multiprocessing to reduce the latency of loading a large image. So long as the image format and loader supports loading regions of interest then we can use different processes or threads to simultaneously get different parts of the image into memory. \n",
    "\n",
    "Often, the image loading can be the bottleneck that slows the whole processing pipeline down. Of course it is often not just the loading that we need to do. There may be a need to do some preprocessing on the image and, for digital pathology, we may need to threshold each region to ensure that we are not wasting time processing empty or background regions of the Whole Slide. There may also be some image transformation or augmentation to do. All of these operations can become the part that slows everything else down and results in under-utilised GPUs, if not dealt with efficiently.\n",
    "\n",
    "We are going to work through an example in which we use a well-known network architecture - the [variational autoencoder](https://en.wikipedia.org/wiki/Variational_autoencoder#:~:text=In%20machine%20learning%2C%20a%20variational,models%20and%20variational%20Bayesian%20methods.https://en.wikipedia.org/wiki/Variational_autoencoder#:~:text=In%20machine%20learning%2C%20a%20variational,models%20and%20variational%20Bayesian%20methods.) - to learn how to distill each patch into a compact representation and then reconstruct the input from this compact representation to test just how well the network is performing. This technique can be a really useful way of converting an unwieldy high-dimensional image into a more compact set of features, which are sufficient for certain analyses. Alternatively, we could simply downsample the image, but this process gets rid of a lot of important, fine-detailed information. On the other hand, because a VAE uses Deep Learning techniques, it is better at retaining the important information and discarding the less useful information (depending on the loss function used). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by importing the libraries we need Python to be able to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from cucim import CuImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will import a couple of Dask-related dependencies and then create a local Dask 'cluster'. As you will see in the cell below we can specify a port with which to connect to a dashboard. We can also specify whether our cluster should use processes or threads (in the same way as they were used in the previous notebook) i.e. 'processes=True' means that we will be using multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import as_completed\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Setup a local cluster.\n",
    "cluster = LocalCluster(dashboard_address= 8789, processes=True)\n",
    "client = Client(cluster)\n",
    "client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that once a client hase been created for the Dask cluster we can output some information about it, such as the total number of threads and workers that are available to the cluster. We can also use the address of the dashboard to access the metrics that Dask's dashboard provides.\n",
    "\n",
    "When you are connected to a local machine then you can just copy and paste the Dashboard URL shown above into your browser or into the dashboard's URL search box at the top.\n",
    "However, because we are connected to a cloud instance, you will need to paste a slightly different URL. This is formed by copying the first part of the address that is showing up for this page in your browser and then appending a semicolon, the port (8789) and the suffix '/status'. \n",
    "N.B. The URL may look a little different but you should take the part up to and including the .com\n",
    "\n",
    "e.g. http://ec2-54-146-232-42.compute-1.amazonaws.com/lab/lab/tree/Notebook_2.ipynb\n",
    "\n",
    "would be truncated to 'http://ec2-54-146-232-42.compute-1.amazonaws.com' and appended to  '[:8789/status](http://ec2-54-146-232-42.compute-1.amazonaws.com)'\n",
    "\n",
    "to give: http://ec2-3-216-79-138.compute-1.amazonaws.com:8789/status\n",
    "\n",
    "If you paste your version of this into the Dask Dashbaord URL you should see that the various dashboard options will appear as orange rectangles. Each time you click on one a new tab will be created with than specific metric displayed. You can drag and drop them into a panel on the right of this notebook. It is recommended to include the 'Progress' and 'Task Stream' items, which will show the progress of some later computations.\n",
    "\n",
    "![Dask](images/dask.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can now load a thumbnail of the image to show what we will be working with. In this case we are working with an image from the [CAMELYON 17 challenge dataset](https://camelyon17.grand-challenge.org/data/https://camelyon17.grand-challenge.org/data/). The dataset  is open access and can be downloaded from a number of sources and are made available under [CCO](https://creativecommons.org/publicdomain/zero/1.0https://creativecommons.org/publicdomain/zero/1.0) \n",
    "\n",
    "The dataset contains whole-slide images (WSI) of hematoxylin and eosin (H&E) stained lymph node sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image header\n",
    "input_file = \"data/patient_100_node_0.tif\"\n",
    "wsi = CuImage(input_file)\n",
    "\n",
    "# Extract the image metadata\n",
    "sizes=wsi.metadata[\"cucim\"][\"resolutions\"]\n",
    "levels = sizes[\"level_count\"]\n",
    "\n",
    "# Dimensions of the whole Slide at full-resolution\n",
    "w = sizes[\"level_dimensions\"][0][0]\n",
    "h = sizes[\"level_dimensions\"][0][1]\n",
    "\n",
    "# Dimensions of the Whole Slide at lowest resolution\n",
    "wt = sizes[\"level_dimensions\"][levels-1][0]\n",
    "ht = sizes[\"level_dimensions\"][levels-1][1]\n",
    "\n",
    "# Load and decode the pixels at the lowest resolution\n",
    "wsi_thumb = wsi.read_region(location=(0,0), size=(wt,ht), level=levels-1)\n",
    "\n",
    "# Show the image\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wsi_thumb)\n",
    "plt.title('patient_100_node_0.tif')\n",
    "print(\"Width = {}, Height = {}\".format(w, h))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you should notice that the majority of the image is actually empty background. This means that any processing we do on those regions is not going to provide any useful information and, because of the size of the image (with billions of pixels) we will need to split the image into patches to ingest it into our VAE.\n",
    "\n",
    "The traditional way of accomplishing this might be to pre-tile the images, saving their coordinates in some sort of metadata file and discarding the empty regions. There are many different algorithms to ascertain whether each patch might be useful foreground information or uninformative background, but for this exercise we will use the variance of the pixel intensities. If all the pixels are the same, then the variance will be 0 - regardless of the specific pixel intensity (for each color channel).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "Create a function that accepts a 3D array (Width, Height, Color Channels) and returns True if the input is above a fixed threshold and False if below it. The skeleton is provided below. Remember that we have 2 dimensions and 3 channels and we need to compute the variance across all of these values ([solution](solutions/solution2_1.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates whether the block contains tissue to analyse\n",
    "def threshold(arr, threshold_value):\n",
    "    \n",
    "    # TODO - check whether there is sufficient variance in the input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test this function we will load a few patches from the test image above and see whether they produce the correct result. You may need to change the default threshold value to get the required result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch1 = np.array(wsi.read_region(location=(41500,30000), size=(64,64), level=levels-1))\n",
    "patch2 = np.array(wsi.read_region(location=(41500,2000), size=(64,64), level=levels-1))\n",
    "patch3 = np.array(wsi.read_region(location=(80000,78000), size=(64,64), level=levels-1))\n",
    "\n",
    "patch1_result = threshold(patch1)\n",
    "patch2_result = threshold(patch2)\n",
    "patch3_result = threshold(patch3)\n",
    "\n",
    "print(\"patch1 > threshold is {}\".format(patch1_result))\n",
    "print(\"patch2 > threshold is {}\".format(patch2_result))\n",
    "print(\"patch3 > threshold is {}\".format(patch3_result))\n",
    "      \n",
    "fig, ax = plt.subplots(1,3,figsize = (10,10))\n",
    "ax[0].imshow(patch1)\n",
    "ax[1].imshow(patch2)\n",
    "ax[2].imshow(patch3)\n",
    "\n",
    "if patch1_result == patch1_result == True and patch3_result == False:\n",
    "    print(\"Correct!\")\n",
    "else:\n",
    "    print(\"Not quite...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, hopefully, we now have a basic working threshold function. The next task is to create a pipeline to look at the whole slide and emit a list of all the tiles that are above the threshold so that we can do something with them.\n",
    "\n",
    "In this next step we are going to introduce [DASK](https://docs.dask.org/en/stable/https://docs.dask.org/en/stable/), which is a very useful tool for breaking large tasks into lots of smaller chunks to reduce overall latency. This may sound quite familiar - because that is precisely what we were dong with Python's multiprocessing and multithreading tools in the previous exercises. DASK provides features that resemble some of these functions, but it also provides a swathe of other benefits including:\n",
    "\n",
    "* A rich set of visualization tools to monitor the status of your running code\n",
    "* Integrations and compatibility with many other tools from the Data Science ecosystem\n",
    "* Abstractions that provide powerful but simple to use concurrency\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to concurrency, DASK provides two main tools - [Futures](https://docs.dask.org/en/stable/futures.htmlhttps://docs.dask.org/en/stable/futures.html) and [Delayed](https://docs.dask.org/en/stable/delayed.htmlhttps://docs.dask.org/en/stable/delayed.html) functions.\n",
    "\n",
    "Futures are used to asynchronously process results, with the results becoming available when the computation has completed. Delayed functions are used to 'lazily' compute values, as the results of prior computations or inputs become available.\n",
    "\n",
    "Let's look at a toy example. Imagine that we want to sum a series of integers. Naively, you'd have to iterate over each element one at a time adding each element to the running total. The run time would be a factor of the number elements. A better way would be to add every other element to its neighbour iteratively until there is only one element left. This would bring the runtime down to log(N) time. By providing a few basic commands you can let Dask figure out the execution graph for you. Let's look at a concrete example\n",
    "\n",
    "We can write the code to do the adding for us using a Dask Delayed function. This means that before the result is calculated a graph is constructed and Dask will map this graph onto the available compute (e.g. Processes, Threads or GPUs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask import delayed\n",
    "\n",
    "@dask.delayed\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "a = [1,2,3,4,5,6,7,8,9,10,11,12,13,15,15,16]\n",
    "b = []\n",
    "c = []\n",
    "d = []\n",
    "\n",
    "for i in range(0,16,2):\n",
    "    b.append(add(a[i],a[i+1]))\n",
    "    \n",
    "for i in range(0,8,2):\n",
    "    c.append(add(b[i],b[i+1]))\n",
    "    \n",
    "for i in range(0,4,2):\n",
    "    d.append(add(c[i],c[i+1]))\n",
    "    \n",
    "e=add(d[0],d[1])\n",
    "    \n",
    "e.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, no computation has been done - just the graph construction. By doing this up-front, a more efficient graph can be created. You can see that the graph shows how the additions at each phase can be done in parallel , but also how each subsequent addition depends only on its ancestors. To actually do the computation, we need to execute a compute() command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the same technique to the task of loading the Whole Slide Image. In this example we will use Dask Futures. To do this we create batches of inputs to process and let Dask map them to available compute resources using the map function. Because this executes asynchronously, we need to wait until each individual task has completed before handing the result on for further processing. In this case, we will use the as_completed function, which yields results as they come in. \n",
    "This sort of dynamic execution allows for more efficient concurrent execution, which can significantly reduce overall latency.\n",
    "\n",
    "Normally, when processing a Whole Slide Image, you'd probably threshold at a reduced resolution but in this case we are going to do it at full resolution, but break it into 2 steps. First of all we will threshold the whole image in 256 x 256 patches, then we will add the coordinates of those patches above the threshold to a list. When this stage has completed we will then do a second thresholding, but this time from 64x64 tiles within the list of above-threshold patches.\n",
    "\n",
    "At the end of the process we will have a single list containing all the 64x64 tiles that are of interest for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 256\n",
    "tile_size = 64\n",
    "\n",
    "# iterate over a set of regions from which to threshold\n",
    "def process_patch(start_loc_list):\n",
    "    ps = patch_size\n",
    "    slide = CuImage(input_file)\n",
    "    res = []\n",
    "    for start_loc in start_loc_list:\n",
    "        # you can do usually do thresholding at higher reduction factor\n",
    "        level = 0\n",
    "        region = np.array(slide.read_region(start_loc, [ps , ps], level))\n",
    "        if threshold(region):\n",
    "            res.append((start_loc[0], start_loc[1]))\n",
    "        \n",
    "    return res\n",
    "\n",
    "# As the results are processed, put them into a list\n",
    "def compile_results(futures):\n",
    "    patches = []\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        res1 = future.result()\n",
    "        if res1:\n",
    "            for patch in res1:\n",
    "                patches.append(patch)\n",
    "                \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we execute the cell below it will run through each patch, and evaluate whether it is above the threshold we defined earlier. To reduce latency we let Dask map the various tiles to evaluate to the available workers (which might be threads, processes or GPU nodes).\n",
    "\n",
    "Notice that the dashboard will show the status of each task and the state of each worker as the computation unfolds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "patch_size = 256\n",
    "num_chunks = 64\n",
    "\n",
    "start_loc_data = [(sx, sy)\n",
    "                  for sy in range(0, h, patch_size)\n",
    "                      for sx in range(0, w, patch_size)]\n",
    "\n",
    "chunk_size = len(start_loc_data) // num_chunks\n",
    "\n",
    "start_loc_list = [start_loc_data[i:i+chunk_size]  for i in range(0, len(start_loc_data), chunk_size)]\n",
    "future_result1 = list(client.map(process_patch, start_loc_list))\n",
    "patches = compile_results(future_result1)\n",
    "                 \n",
    "print(\"Number of 256 x 256 patches found = {}\".format(len(patches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we just thresholded the whole WSI at full resolution!\n",
    "\n",
    "### Task 2\n",
    "\n",
    "For a finer-grained threshold on each 64x64 tile within these blocks, create a second phase of computation that will process the patches and output a similar list of (64x64) tiles. You should use the previous tile stage as a reference ([solution](solutions/solution2_2.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# TODO iterate over a set of patches from which to threshold\n",
    "def process_tile(...):\n",
    "    pass\n",
    "\n",
    "# TODO create a list of tiles to threshold from each patch returned\n",
    "\n",
    "# TODO map the tiles to threshold to the process tile function\n",
    "\n",
    "# TODO process the results\n",
    "tiles = compile_results(...)\n",
    "\n",
    "print(\"Number of 64 x 64 tiles found = {}\".format(len(tiles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see about 408,915 tiles (depending on how you set the threshold value)\n",
    "\n",
    "Now we can load a few random tiles to check that they look okay. Hopefully they all contain some tissue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize = (15,15))\n",
    "i = random.randint(0,len(tiles)-1)\n",
    "ax[0].imshow(tiles[i][2])\n",
    "i = random.randint(0,len(tiles)-1)\n",
    "ax[1].imshow(tiles[i][2])\n",
    "i = random.randint(0,len(tiles)-1)\n",
    "ax[2].imshow(tiles[i][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common technique for combatting the huge amount of data we need to ingest for a single whole slide image is to reduce each tile down to a much smaller set of features that capture the essence of the tile. We could use some statistical measures such as mean pixel intensity, variance etc. The problem is that these features may not capture the unique nature of the images we are using here. \n",
    "\n",
    "Instead, what we will do is to use a Variational AutoEncoder that has been trained in an unsupervised way to encode and then decode each tile, using a loss function that compares the original image to the decoded version. Once trained, we then just retain the encoder part of the network, which has reduced our inputs down to just 32 features (aka latent variables). These features effectively represent the principle components of the tiles.\n",
    "\n",
    "This can make analysis of a Whole Slide a more tractable problem because we can hugely reduce the amount of information to process. By removing the background data and then converting each 64x64 tile into a 32x1 tensor we have reduced the information on this slide by a factor of about 12,000. That's quite a significant reduction but we are still left with 11,848,000 data points.\n",
    "\n",
    "In this instance we are not going to train the VAE because it would take too long but you can examine the PyTorch model code and we will run it to do inference on a few tiles, so that you can see what its output looks like.\n",
    "\n",
    "![VAE](images/VAE.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to build the VAE model in PyTorch\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=1024):\n",
    "        return input.view(input.size(0), size, 1, 1)\n",
    "    \n",
    "class VAE2(nn.Module):\n",
    "    def __init__(self, image_channels=3, h_dim=1024, z_dim=32):\n",
    "        super(VAE2, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(z_dim, h_dim)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UnFlatten(),\n",
    "            nn.ConvTranspose2d(h_dim, 128, kernel_size=5, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, image_channels, kernel_size=6, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        return z, mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.fc3(z)\n",
    "        #print(z.shape)\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        z = self.decode(z)\n",
    "        return z, mu, logvar\n",
    "    \n",
    "model2 = VAE2(image_channels=3)\n",
    "model2 = torch.nn.DataParallel(model2, device_ids=[0])\n",
    "model2.cuda()\n",
    "\n",
    "reconstruction_function = nn.BCELoss()\n",
    "reconstruction_function.size_average = False\n",
    "def loss_function2(recon_x, x, mu, logvar):\n",
    "    BCE = reconstruction_function(recon_x, x)\n",
    "\n",
    "    # https://arxiv.org/abs/1312.6114 (Appendix B)\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    KLD /= 1600 * 32 * 32\n",
    "    return BCE + KLD\n",
    "\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can test the trained VAE with a few samples. To do this we can load the weights from a previously trained model which were saved into the 'M2_WEIGHTS.PT' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = VAE2(image_channels=3)\n",
    "model2 = torch.nn.DataParallel(model2, device_ids=[0])\n",
    "dct = torch.load('data/M2_WEIGHTS.PT')\n",
    "model2.load_state_dict(dct['state_dict'])\n",
    "model2.cuda()\n",
    "\n",
    "model2.eval()\n",
    "\n",
    "bs = len(tiles) // 128\n",
    "\n",
    "batch_list = [i for i in range(len(tiles))]\n",
    "random.shuffle(batch_list)\n",
    "batch=np.zeros((512,3,tile_size,tile_size),np.uint8)\n",
    "\n",
    "if len(batch_list)>=512:\n",
    "\n",
    "    for p in range(512):\n",
    "        j = batch_list.pop()\n",
    "        tile = np.moveaxis(tiles[j][2],2,0)\n",
    "        batch[p]=tile\n",
    "        \n",
    "\n",
    "    tiles_cuda=torch.FloatTensor(batch).cuda()\n",
    "    tiles_cuda=tiles_cuda/255\n",
    "\n",
    "    recon_batch, _, _ = model2(tiles_cuda)\n",
    "\n",
    "    tensor1 = recon_batch.cpu().detach().numpy() * 255\n",
    "    tensor1 = tensor1.astype(np.uint8)\n",
    "    tensor2 = tiles_cuda.cpu().numpy() * 255\n",
    "    tensor2 = tensor2.astype(np.uint8)\n",
    "    \n",
    "    print(\"Real Inputs\")\n",
    "    fig, ax = plt.subplots(1,4,figsize = (10,10))\n",
    "    ax[0].imshow(np.moveaxis(tensor2[20,:,:,:],0,2))\n",
    "    ax[1].imshow(np.moveaxis(tensor2[25,:,:,:],0,2))\n",
    "    ax[2].imshow(np.moveaxis(tensor2[120,:,:,:],0,2))\n",
    "    ax[3].imshow(np.moveaxis(tensor2[127,:,:,:],0,2))\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Regenerated\")\n",
    "    fig, ax = plt.subplots(1,4,figsize = (10,10))\n",
    "    ax[0].imshow(np.moveaxis(tensor1[20,:,:,:],0,2))\n",
    "    ax[1].imshow(np.moveaxis(tensor1[25,:,:,:],0,2))\n",
    "    ax[2].imshow(np.moveaxis(tensor1[120,:,:,:],0,2))\n",
    "    ax[3].imshow(np.moveaxis(tensor1[127,:,:,:],0,2))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, with only 32 values the VAE can do a reasonable job of representing each tile. This VAE has only been trained on a couple of WSIs but with more training (and more latent variables) you could get a much clearer rendering of the original. For this exercise we are not going to worry too much about the quality of the VAE.\n",
    "\n",
    "For now, what has been done is that a Pandas Dataframe has been saved with the location of all of the tiles and the 32 latent variables computed by the VAE for each of those tiles. In the next notebook we will load up this Dataframe and see how it can be used for a variety of analyses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
