{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac592da",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using MONAI to unlock clinically valuable insights from Digital Pathology\n",
    "\n",
    "## Dealing with the size of Whole Slide Images\n",
    "\n",
    "Today’s image acquisition devices, be they digital pathology slide scanners or lightsheet microscopes, can generate a huge amount of data. This volume of data can make it very challenging to move around, save and load - let alone trying to ingest it into some sort of machine-learning or deep learning algorithm. \n",
    "\n",
    "The objective of this workshop is to introduce you to a few tools and techniques that can really help to deliver insights from this rich data without exhausting your system memory or taking eons to run. In fact, you may be surprised to see that outputs from MONAI can be used with the GPU accelerated RAPIDS API to turn some, previously unfeasible, analyses into near-real-time processes.\n",
    "\n",
    "This workshop will mostly focus on digital pathology, but really, these techniques are very generic and could be applied to data from many different modalities.\n",
    "\n",
    "Images or volumes can be saved in a variety of formats, some of which are generic and some of which are domain-specific. Additionally, images may be saved and loaded using formats that are based on open-standards or are proprietary to the manufacturers of the device used to capture the image.\n",
    "\n",
    "The image that we will be using has come from the TCGA archive (https://www.cancer.gov/tcga) and was saved in the .svs format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e26f661",
   "metadata": {},
   "source": [
    "## Part 1 - Loading images\n",
    "\n",
    "Images store a lot of information. Most commonly, images are composed of one or more channels of intensity values across 2 or 3 dimensions. In order to keep file sizes manageable, compression is usually employed. In some cases lossy compression is suitable but for other domains the images need to be lossless. This means that getting all of the pixel data from disk into computer memory can be quite an intense process and without the right tools, techniques and hardware, it can be a slow process. If you have an accelerator such as a GPU you may find that you are unable to utilize its full capabilities because you are unable to feed it data at a sufficiently high rate to keep it busy. \n",
    "This first section introduces a few tools that you can use to make best use of the resources available when it comes to loading the data. There are a few factors that come into play here:\n",
    "- The efficiency of the software algorithm\n",
    "- The speed of the machine-code that the software is compiled into\n",
    "- The number of CPU threads or processes used\n",
    "- The performance of the disk and networking that the data needs to traverse.\n",
    "- The speed of the CPU\n",
    "- Any hardware that the CPU supports to accelerate certain processes, such as AVX instructions\n",
    "\n",
    "For the loading of a variety of biomedical imaging formats, the go-to software has been OpenSlide, which can load formats such as Aperio’s .svs format and many other tiff-based formats. First of all, let’s use OpenSlide to load up one of the images we have to get a feel for the latency involved in loading images at certain resolutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8610cd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openslide\n",
    "\n",
    "# Load the image\n",
    "slide = openslide.OpenSlide(\"data/tcga1.svs\")\n",
    "\n",
    "# Get the dimensions at level 0 (Full size)\n",
    "width, height = slide.level_dimensions[0]\n",
    "\n",
    "print(\"Full-Size Image Dimensions - Width = {}, Height = {}\".format(width, height))\n",
    "\n",
    "print(\"Level Downsamples - {}\".format(slide.level_downsamples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41db812",
   "metadata": {},
   "source": [
    "You should see that this image is 87647 x 52434, so that’s 4.6 billion pixels, with 3 color channels - just in one image. To put this into perspective, at a standard display resolution of 120 dots per inch, you’d need a 27 x 20 metre monitor to view this image at full resolution - that's about 2 tennis courts!\n",
    "\n",
    "For this very reason, these types of image are often saved in formats that allow the image to be loaded at a lower resolution or provide a means of only loading a small sub-region of the image.\n",
    "\n",
    "You should also notice that the Whole Slide Image doesn't just contain the full resolution image but also a pyramid of resolutions (The Level Downsamples). In this case, along with the full resolution image, there are also 4x, 16x and 32x down-sampled versions of the image. This permits viewers to load the image at lower resolutions for a broad overview and then the user can choose to zoom in to specific regions, using the higher resolution versions. The more levels that the pyramid contains, the smoother the zooming will be - at the cost of larger file sizes.\n",
    "\n",
    "As we will see later in this notebook, these lower resolution views can also be used for eliminating the empty regions from processing by applying some sort of thresholding function to them and only selecting tiles for inference (or training) from the foreground (tissue) regions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df03aee",
   "metadata": {},
   "source": [
    "![Pyramid](images/pyramid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaeca44",
   "metadata": {},
   "source": [
    "Next we are going to load the image at the lowest resolution in the pyramid - by specifying _slide.level_count-1_ for the _level_ parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747cdc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get height and width at the lowest level resolution\n",
    "w_thumbnail, h_thumbnail = slide.level_dimensions[slide.level_count-1]\n",
    "\n",
    "# Load up the image data at the lowest resolution - to preview it\n",
    "img = slide.read_region((0,0), slide.level_count-1,(w_thumbnail, h_thumbnail))\n",
    "print(\"Reduced-Size Image Dimensions - Width = {}, Height = {}\".format(w_thumbnail,h_thumbnail))\n",
    "\n",
    "# Use Matplotlib to display the thumbnail view of the image\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img)\n",
    "plt.title('tcga1.svs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2221bbc",
   "metadata": {},
   "source": [
    "The image displayed shows that many of the image pixels are actually not very informative. A lot of the image is white background, since this image contains a single tissue slice that is centred on the slide. \n",
    "\n",
    "So let’s investigate the loading time for different resolutions of this image. In the cells below you will see some skeleton code which you need to flesh out to measure the time it takes to load the image at each of the resolutions that it contains. You should complete the code so that it plots the times for each resolution.\n",
    "\n",
    "time_loading_at_resolution is a function that takes a slide and a reduction level and returns the time it takes to load the image data at that resolution\n",
    "\n",
    "Check the [solution](solutions/loading_at_resolution.py) if you get stuck\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc853727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "def time_loading_at_resolution(slide, level): # slide \n",
    "    \n",
    "    start = timer()\n",
    "    \n",
    "    # TODO - insert code to print out the dimensions and load the image at the specified level\n",
    "\n",
    "    end = timer()\n",
    "    \n",
    "    return end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1394178",
   "metadata": {},
   "source": [
    "When you have completed the code in the cell above and run it, you can test it by running the cell below. It should print out a range of load times for different resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c93ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise a list to hold the loading times\n",
    "times = [0] * (slide.level_count-1)\n",
    "\n",
    "# Now call the timing function for a range of possible resolutions\n",
    "for i in range(slide.level_count-1,0,-1):\n",
    "    times[i-1] = time_loading_at_resolution(slide,i)\n",
    "    print(\"Time at resolution reduction level {} = {}\".format(i, times[i-1]))\n",
    "    \n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d2dddf",
   "metadata": {},
   "source": [
    "If that works as expected, you should now have the load times in the array we created, which we can plot out by running the cell below. Note that we are not loading the image at full resolution (level 0) because that would take a long time (and requires sufficient memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42544def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot the load times\n",
    "downsample_factor = slide.level_downsamples[1:4]\n",
    "\n",
    "plt.plot(downsample_factor, times, '-ok')\n",
    "plt.xlabel(\"Reduction Factor\")\n",
    "plt.ylabel(\"Load Time (s)\");\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f327a91",
   "metadata": {},
   "source": [
    "So, because the number of pixels doubles for each doubling of a dimension size, for a 2D image, the load time is quadratically related to the downsample level. As mentioned, we avoided loading the image at reduction level 0 because, according to this trend, it might take several minutes at full resolution.\n",
    "\n",
    "So, what can we do to reduce this load time? One technique that is often used to speed up many different types of operation is to use multi-threading. Multi-threading is a technique in which the process or program running your code spawns multiple sub-processes, known as threads, which can then operate in parallel, reducing the overall time to perform certain operations. In this case, we could get multiple threads loading different parts of the image. However, Python has a mechanism to prevent issues caused by concurrent execution (e.g. data races), known as the Global Interpreter (aka GIL) and this can prevent optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91da3f3",
   "metadata": {},
   "source": [
    "## Introducing CuCIM\n",
    "\n",
    "When dealing with much larger images, it is necessary to utilise as much of the available compute power that we have to run in parallel, otherwise it can be difficult to keep the GPU busy all the time. The problem we face here is that it is not just the Python GIL that we are working with but OpenSlide itself is not especially fast at this sort of operation. For this reason, the cuCIM library was  added to the RAPIDS platform and is also used in MONAI. cuCIM offers similar capabilities to Openslide but has been optimised for the scenario we are exploring. The API is similar, but not exactly the same as OpenSlide, so you can see that, to do what we did before, we will need to amend the loading code slightly. Have a look at the code cell below to see how to get the image dimensions at a specific resolution and load the image.\n",
    "\n",
    "N.B. When loading a specific region of interest at a reduction level > 0, you need to supply the x and y coordinates at the full resolution, whereas the width and height should be supplied at the reduced size. See the [documentation](https://docs.rapids.ai/api/cucim/stable/api.html#module-cucim.CuImage) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa02da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cucim import CuImage\n",
    "\n",
    "input_file = \"data/tcga1.svs\"\n",
    "# load the image header\n",
    "wsi = CuImage(input_file)\n",
    "\n",
    "# Get the resolution meta data\n",
    "sizes=wsi.metadata[\"cucim\"][\"resolutions\"]\n",
    "levels = sizes[\"level_count\"]\n",
    "\n",
    "# Get the dimensions at the lowest resolution level\n",
    "wt = sizes[\"level_dimensions\"][levels-1][0]\n",
    "ht = sizes[\"level_dimensions\"][levels-1][1]\n",
    "\n",
    "# Load the image data at this resolution\n",
    "wsi_thumb = wsi.read_region(location=(0,0), size=(wt,ht), level=levels-1)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wsi_thumb)\n",
    "plt.title('tcga1.svs')\n",
    "print(wt,ht)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972b1644",
   "metadata": {},
   "source": [
    "Now we can compare the performance of image loading using OpenSlide and cuCIM. In the code cell below add the necessary steps for cucim to load the image at the specified resolution ([solution](solutions/load_resolution.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a812039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "def time_loading_at_resolution(level, use_cucim):\n",
    "    \n",
    "    start = timer()\n",
    "\n",
    "    if use_cucim:\n",
    "        sizes=wsi.metadata[\"cucim\"][\"resolutions\"]\n",
    "\n",
    "        # Get the dimensions at the lowest resolution level\n",
    "        wt = sizes[\"level_dimensions\"][level][0]\n",
    "        ht = sizes[\"level_dimensions\"][level][1]\n",
    "\n",
    "        # TODO insert code to load the image at the specified resolution reduction level  \n",
    "        # and with the full width and height at that resolution\n",
    "        wsi_thumb = wsi.read_region(location=(0,0), size=(wt,ht), level=level)\n",
    "    else:\n",
    "        width, height = slide.level_dimensions[level]\n",
    "        img = slide.read_region((0,0), level, (width, height))\n",
    "\n",
    "    end = timer()\n",
    "    \n",
    "    return end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f368892",
   "metadata": {},
   "source": [
    "Once you have completed and run the code cell above, you can run the code below to test the function and generate some load times to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d5d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now call the timing function for each of the possible resolutions\n",
    "cu_times = [0] * (slide.level_count-1)\n",
    "times = [0] * (slide.level_count-1)\n",
    "\n",
    "print(\"Using cuCim...\")\n",
    "for i in range(slide.level_count-1,0,-1):\n",
    "    cu_times[i-1] = time_loading_at_resolution(i,True)\n",
    "    print(\"Time at resolution reduction level {} = {}\".format(i, cu_times[i-1]))\n",
    "\n",
    "print(\"Using OpenSlide...\")\n",
    "for i in range(slide.level_count-1,0,-1):\n",
    "    times[i-1] = time_loading_at_resolution(i,False)\n",
    "    print(\"Time at resolution reduction level {} = {}\".format(i, times[i-1]))\n",
    "\n",
    "print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7950b64b",
   "metadata": {},
   "source": [
    "When it says 'Completed', let's plot that out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd21ccef",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction_factor = slide.level_downsamples[1:4]\n",
    "\n",
    "plt.plot(reduction_factor,times, '-ok')\n",
    "plt.plot(reduction_factor,cu_times, '-or')\n",
    "plt.xlabel(\"Reduction Factor\")\n",
    "plt.ylabel(\"Load Time (s)\")\n",
    "plt.yscale(\"log\")\n",
    "\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8e22a9",
   "metadata": {},
   "source": [
    "So, you should notice that CuCIM is about an order of magnitude faster at loading the image data (note the log scale on the y-axis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42ab3e5",
   "metadata": {},
   "source": [
    "cuCIM includes a feature (since v21.12.1) that actually uses multiple threads internally to load an image. This is a much more efficient and cleaner way of quickly loading an image. It requires no Python GIL workarounds and uses far fewer resources. Let's compare it with our Python implementation.\n",
    "\n",
    "Please note that we are loading a very large image and so there is a chance that we will run out of RAM when loading images at full resolution in this memory-limited environment. If this happens you will most likely see an error message pop up telling you that the kernel just re-launched. If this happens, it will actually remove all the current data from RAM from the previous cells and it will probably work if you try again (No need to re-run any previous cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6bc794",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from cucim import CuImage\n",
    "\n",
    "input_file = \"data/tcga1.svs\"\n",
    "sizes=wsi.metadata[\"cucim\"][\"resolutions\"]\n",
    "width = sizes[\"level_dimensions\"][0][0]\n",
    "height = sizes[\"level_dimensions\"][0][1]\n",
    "img = wsi.read_region((0,0),(width, height), 0, num_workers=16)\n",
    "print(img.shape)\n",
    "del(img) # reclaim the memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c339691d",
   "metadata": {},
   "source": [
    "Now we can try loading the image with OpenSlide but use different threads to load different parts of the image. Although the run time is slower than cuCIM, this is not a particularly fair comparison since in the simplistic OpenSlide code, there was no ability to actually assign the sub regions loaded into a single global array. Also, the speed-up you get is affected by the number of CPU cores available and the Cloud instances that are hosting these sessions typically only have a handful of virtual cores. You will see better performance gains on a higher-spec workstation or server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ca2b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# You can also try changing the number of threads and see what effect\n",
    "# it has on the overall run time\n",
    "num_threads = 16\n",
    "level = 0\n",
    "\n",
    "class loaderThread (threading.Thread):\n",
    "    def __init__(self, threadID):\n",
    "        # Class initialisation - set its ID\n",
    "        threading.Thread.__init__(self)\n",
    "        self.threadID = threadID\n",
    "\n",
    "    def run(self):\n",
    "        print(\"Starting thread {}\".format(self.threadID))\n",
    "        start = timer()\n",
    "        width, height = slide.level_dimensions[level]\n",
    "        x = (width // num_threads) * (1-self.threadID)\n",
    "        img = slide.read_region((x,0), level,(width//num_threads, height))\n",
    "        end = timer()\n",
    "        print(\"Exiting thread {}, running time = {}\".format(self.threadID, str(end-start)))\n",
    "\n",
    "threads = []\n",
    "\n",
    "for i in range(num_threads):\n",
    "    # Create new threads\n",
    "    thread = loaderThread(i)\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "# Wait for all threads to complete\n",
    "for t in threads:\n",
    "    t.join()\n",
    "    \n",
    "print(\"Exiting Main Thread\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f31e7",
   "metadata": {},
   "source": [
    "So, despite the unfair comparison, cuCIM should have loaded the whole image in a shorter time because it is using multiple threads to load the data concurrently and because this is happening at C++ layer, there is no GIL problem to slow things down. You can also see that the output reports the total CPU time as ~50 seconds - which is how long it might have taken using a single thread.\n",
    "\n",
    "What is also useful is that although cuCIM uses concurrent threads to load separate regions, it stitches them all together into one array\n",
    "\n",
    "N.B. If you do keep running out of memory, you can load the image at a reduction level of 1 instead of 0 (full resolution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c08a2a",
   "metadata": {},
   "source": [
    "This section should have given you a good grasp of how much of a difference the combination of a decent image loader and some threading or multi-processing can make."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72911bad",
   "metadata": {},
   "source": [
    "**Using Dask instead of doing threading ourselves**\n",
    "\n",
    "In this next step we are going to introduce [DASK](https://docs.dask.org/en/stable/https://docs.dask.org/en/stable/), which is a very useful tool for breaking large tasks into lots of smaller chunks to reduce overall latency. For many Python developers, this is a preferable alternative to directly working with multiprocessing and multithreading APIs in the previous exercises. DASK provides features that resemble some of these functions, but it also provides a swathe of other benefits including:\n",
    "\n",
    "* A rich set of visualization tools to monitor the status of your running code\n",
    "* Integrations and compatibility with many other tools from the Data Science ecosystem\n",
    "* Abstractions that provide powerful but simple to use concurrency\n",
    "\n",
    "When it comes to concurrency, DASK provides two main tools - Futures and Delayed functions.\n",
    "\n",
    "Futures are used to asynchronously process results, with the results becoming available when the computation has completed. Delayed functions are used to 'lazily' compute values, as the results of prior computations or inputs become available.\n",
    "\n",
    "Let's see how we could use this to threshold a Whole Slide Image (i.e. remove empty background regions)\n",
    "\n",
    "First, we create a Dask cluster..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f1446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import as_completed\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# Setup a local cluster.\n",
    "cluster = LocalCluster(dashboard_address= 8789, processes=True)\n",
    "client = Client(cluster)\n",
    "client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3dd962",
   "metadata": {},
   "source": [
    "Next we define a function to check whether there is any tissue in a passed-in image tile. Variance is a fairly good proxy and is also invariant to intensity, which means that we don't need to establish a unique threshold for each individual image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab6e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates whether the block contains tissue to analyse\n",
    "def threshold(arr, threshold_value=80):\n",
    "    \n",
    "    # check whether there is sufficient variance in the input\n",
    "    if arr.flatten().var() > threshold_value:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9292271f",
   "metadata": {},
   "source": [
    "Next we define a patch size and get the downsample factor used at level 1. There is no need to threshold at full resolution in this case.\n",
    "\n",
    "We also define a function that takes a list of coordinates and evaluates each tile using the previously defined threshold function. If the result is above threshold, then the coordinates get added to a results list, otherwise they are discarded.\n",
    "\n",
    "The compile_results function waits for all the Dask processes to return results and adds them to a global list. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abc6dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "patch_size = 164\n",
    "input_file = \"data/tcga1.svs\"\n",
    "reduction = int(wsi.metadata[\"cucim\"][\"resolutions\"][\"level_downsamples\"][1])\n",
    "\n",
    "# iterate over a set of regions from which to threshold\n",
    "def process_patch(start_loc_list):\n",
    "    \n",
    "    # load the image header\n",
    "    wsi = CuImage(input_file)\n",
    "    res = []\n",
    "    \n",
    "    for start_loc in start_loc_list:\n",
    "        region = np.array(wsi.read_region(location=start_loc, size=[patch_size//reduction , patch_size//reduction], level=1))\n",
    "        if threshold(region):\n",
    "            res.append((start_loc[0], start_loc[1]))\n",
    "        \n",
    "    return res\n",
    "\n",
    "# As the results are processed, put them into a list\n",
    "def compile_results(futures):\n",
    "    patches = []\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        res1 = future.result()\n",
    "        if res1:\n",
    "            for patch in res1:\n",
    "                patches.append(patch)\n",
    "                \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a514320",
   "metadata": {},
   "source": [
    "Now we can actually execute the thresholding steps. We build a list of coordinates that need to be evaluated and then split this list into _num_chunks_ smaller lists and let Dask distribute them between the worker processes in the cluster that we created. The results are accumulated asynchronously by the _compile_results_ function\n",
    "\n",
    "Note that this will take a few tens of seconds to execute. If you load the Dask Toolbox from the left toolbar, you can see the progress of the computation. You'll need to add the URL of the Dask Cluster into the Search box which will be something like http://...courses.nvidia.com:8789 (use the URL of the cloud instance from your browser and append the :8789 port) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d4f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_chunks = 32\n",
    "\n",
    "w = sizes[\"level_dimensions\"][0][0]\n",
    "h = sizes[\"level_dimensions\"][0][1]\n",
    "\n",
    "start_loc_data = [(sx, sy)\n",
    "                  for sy in range(0, h, patch_size)\n",
    "                      for sx in range(0, w, patch_size)]\n",
    "\n",
    "chunk_size = len(start_loc_data) // num_chunks\n",
    "\n",
    "start_loc_list = [start_loc_data[i:i+chunk_size]  for i in range(0, len(start_loc_data), chunk_size)]\n",
    "future_result1 = list(client.map(process_patch, start_loc_list))\n",
    "patches = compile_results(future_result1)\n",
    "                 \n",
    "print(\"Number of patches found = {}\".format(len(patches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea20d711",
   "metadata": {},
   "source": [
    "Let's plot that out now by creating a binary mask and setting the pixels to 1 for all patches that were returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65f57ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = sizes[\"level_dimensions\"][1][0]\n",
    "h = sizes[\"level_dimensions\"][1][1]\n",
    "\n",
    "mask = np.zeros((h//patch_size,w//patch_size),dtype=int)\n",
    "\n",
    "print(mask.shape)\n",
    "\n",
    "for patch in patches:\n",
    "    j = patch[0]//(patch_size*reduction)\n",
    "    i = patch[1]//(patch_size*reduction)\n",
    "    if i< h//patch_size and j < w//patch_size:\n",
    "        mask[i,j]=1\n",
    "    \n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(mask)\n",
    "plt.title('threshold mask')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001dd3cd",
   "metadata": {},
   "source": [
    "Hopefully you can see the outline of the tissue from the WSI. As you will see in the next notebook, this sort of capability could be used during inference or training to eliminate regions of the WSI from unnecessary inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b519a16",
   "metadata": {},
   "source": [
    "**Bonus Material - Toy Dask Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ac25b2",
   "metadata": {},
   "source": [
    "Let's look at a toy example. Imagine that we want to sum a series of integers. Naively, you'd have to iterate over each element one at a time adding each element to the running total. The run time would be a factor of the number elements. A better way would be to concurrently add every other element to its neighbour iteratively until there is only one element left. This would bring the runtime down to log(N) time. By providing a few basic commands you can let Dask figure out the execution graph for you. Let's look at a concrete example\n",
    "\n",
    "We can write the code to do the adding for us using a Dask Delayed function. This means that before the result is calculated a graph is constructed and Dask will map this graph onto the available compute (e.g. Processes, Threads or GPUs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7651d13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask import delayed\n",
    "\n",
    "@dask.delayed\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "a = [i+1 for i in range(16)]\n",
    "b = []\n",
    "\n",
    "while len(a)>1:\n",
    "    for i in range(0,len(a),2):\n",
    "         b.append(add(a[i],a[i+1]))\n",
    "    a=b\n",
    "    b=[]\n",
    "    \n",
    "result = a[0]\n",
    "   \n",
    "result.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ab5deb",
   "metadata": {},
   "source": [
    "At this point, no computation has been done - just the graph construction. By doing this up-front, a more efficient graph can be created. You can see that the graph shows how the additions at each phase can be done in parallel , but also how each subsequent addition depends only on its ancestors. To actually do the computation, we need to execute a compute() command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9705c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34e8d45",
   "metadata": {},
   "source": [
    "Note that, in this toy example, the overhead of organizing the concurrency would far outweight any gains. This technique is really only suitable for larger problems. So, make sure you don't prematurely optimize anything (\"the root of all evil\" according to Donald Knuth!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
